# -*- coding: utf-8 -*-
"""newsgroup.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pqO7gMt3ecm-SviGNZhFGRSNjc3V1d4j

Installing required libraries
"""

import numpy as np
import pandas as pd
import os
from sklearn.feature_extraction.text import TfidfVectorizer

"""Fetching 20 newsgroup data from sklearn and converting it into dataframe"""

from sklearn.datasets import fetch_20newsgroups
newsgroups = fetch_20newsgroups(subset='all')

df = pd.DataFrame(newsgroups.data, columns=['text'])
df['categories'] = [newsgroups.target_names[index] for index in newsgroups.target]
df.head()

"""Intializing standard stopwords and adding few more user defined stopwords"""

import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords # Importing list of stop words from nltk
from string import punctuation # Importing list of punctuations from string
punctuations=list(punctuation)
stopWords=stopwords.words('english')
stopWords+=punctuations # Combined list of stop words

# Common words throughout all docs play no part in classification ,so removing them
stopWords+=['subject:','from:', 'date:', 'newsgroups:', 'message-id:', 'lines:', 'path:', 'organization:', 
            'would', 'writes:', 'references:', 'article', 'sender:', 'nntp-posting-host:', 'people', 
            'university', 'think', 'xref:', 'cantaloupe.srv.cs.cmu.edu', 'could', 'distribution:', 'first', 
            'anyone','world', 'really', 'since', 'right', 'believe', 'still', 
            "max>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'"]

df['cleaned_text']=df['text'].apply(lambda x: [item for item in x.split() if item not in stopWords])

cleaned_df=pd.DataFrame(df['cleaned_text'])

cleaned_df['text']= cleaned_df['cleaned_text'].apply(lambda x: ' '.join(x))

cleaned_df

"""Computing Tf IDF vectors"""

v = TfidfVectorizer()
x = v.fit_transform(cleaned_df['text'])

"""Computing Word2Vec Model"""

cleaned_df1=pd.DataFrame(df['cleaned_text'])

cleaned_df1

from gensim.models import Word2Vec
from nltk.corpus import gutenberg
from multiprocessing import Pool
from scipy import spatial

from gensim.test.utils import common_texts
from gensim.models import Word2Vec

model = Word2Vec(sentences=cleaned_df['cleaned_text'], window=5, min_count=1, workers=4)
model.save("word2vec.model")
model = Word2Vec.load("word2vec.model")
#model.train([["hello", "world"]], total_examples=1, epochs=1)

model.init_sims(replace = True)

cleaned_df1

model.wv.most_similar("car")

def vectorize(list_of_docs, model):
    
    features = []

    for tokens in list_of_docs:
        zero_vector = np.zeros(model.vector_size)
        vectors = []
        for token in tokens:
            if token in model.wv:
                try:
                    vectors.append(model.wv[token])
                except KeyError:
                    continue
        if vectors:
            vectors = np.asarray(vectors)
            avg_vec = vectors.mean(axis=0)
            features.append(avg_vec)
        else:
            features.append(zero_vector)
    return features
    
vectorized_docs = vectorize(cleaned_df['cleaned_text'], model=model)
len(vectorized_docs), len(vectorized_docs[0])

vectorized_docs

"""Key Phrases Implementation"""

pip install keybert

from keybert import KeyBERT
kw_model = KeyBERT()
keywords = kw_model.extract_keywords(cleaned_df['cleaned_text'])

print(kw_model.extract_keywords(cleaned_df['cleaned_text'], keyphrase_ngram_range=(1, 1), stop_words=None))

from keybert import KeyBERT

doc = """
         Supervised learning is the machine learning task of learning a function that
         maps an input to an output based on example input-output pairs. It infers a
         function from labeled training data consisting of a set of training examples.
         In supervised learning, each example is a pair consisting of an input object
         (typically a vector) and a desired output value (also called the supervisory signal).
         A supervised learning algorithm analyzes the training data and produces an inferred function,
         which can be used for mapping new examples. An optimal scenario will allow for the
         algorithm to correctly determine the class labels for unseen instances. This requires
         the learning algorithm to generalize from the training data to unseen situations in a
         'reasonable' way (see inductive bias).
      """

kw_model = KeyBERT()
keywords = kw_model.extract_keywords(doc)
print(kw_model.extract_keywords(doc, keyphrase_ngram_range=(2, 2), stop_words=None))

