{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load toy set data\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n"
     ]
    }
   ],
   "source": [
    "#load data in dataframes\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(iris.data)\n",
    "#df = pd.read_csv(path)\n",
    "print(iris.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "center_0--> [5.00566038 3.36981132 1.56037736 0.29056604]\n",
      "center_1--> [6.05666667 2.79666667 4.48166667 1.44666667]\n",
      "center_2--> [6.6972973  3.03243243 5.73243243 2.1       ]\n",
      "center_0--> [5.006 3.428 1.462 0.246]\n",
      "center_1--> [5.91935484 2.75322581 4.39032258 1.41935484]\n",
      "center_2--> [6.82105263 3.06578947 5.74736842 2.09473684]\n",
      "center_0--> [5.006 3.428 1.462 0.246]\n",
      "center_1--> [5.9016129  2.7483871  4.39354839 1.43387097]\n",
      "center_2--> [6.85       3.07368421 5.74210526 2.07105263]\n",
      "[[5.1 3.5 1.4 0.2 0.  0. ]\n",
      " [4.9 3.  1.4 0.2 0.  0. ]\n",
      " [4.7 3.2 1.3 0.2 0.  0. ]\n",
      " [4.6 3.1 1.5 0.2 0.  0. ]\n",
      " [5.  3.6 1.4 0.2 0.  0. ]\n",
      " [5.4 3.9 1.7 0.4 0.  0. ]\n",
      " [4.6 3.4 1.4 0.3 0.  0. ]\n",
      " [5.  3.4 1.5 0.2 0.  0. ]\n",
      " [4.4 2.9 1.4 0.2 0.  0. ]\n",
      " [4.9 3.1 1.5 0.1 0.  0. ]\n",
      " [5.4 3.7 1.5 0.2 0.  0. ]\n",
      " [4.8 3.4 1.6 0.2 0.  0. ]\n",
      " [4.8 3.  1.4 0.1 0.  0. ]\n",
      " [4.3 3.  1.1 0.1 0.  0. ]\n",
      " [5.8 4.  1.2 0.2 0.  0. ]\n",
      " [5.7 4.4 1.5 0.4 0.  0. ]\n",
      " [5.4 3.9 1.3 0.4 0.  0. ]\n",
      " [5.1 3.5 1.4 0.3 0.  0. ]\n",
      " [5.7 3.8 1.7 0.3 0.  0. ]\n",
      " [5.1 3.8 1.5 0.3 0.  0. ]\n",
      " [5.4 3.4 1.7 0.2 0.  0. ]\n",
      " [5.1 3.7 1.5 0.4 0.  0. ]\n",
      " [4.6 3.6 1.  0.2 0.  0. ]\n",
      " [5.1 3.3 1.7 0.5 0.  0. ]\n",
      " [4.8 3.4 1.9 0.2 0.  0. ]\n",
      " [5.  3.  1.6 0.2 0.  0. ]\n",
      " [5.  3.4 1.6 0.4 0.  0. ]\n",
      " [5.2 3.5 1.5 0.2 0.  0. ]\n",
      " [5.2 3.4 1.4 0.2 0.  0. ]\n",
      " [4.7 3.2 1.6 0.2 0.  0. ]\n",
      " [4.8 3.1 1.6 0.2 0.  0. ]\n",
      " [5.4 3.4 1.5 0.4 0.  0. ]\n",
      " [5.2 4.1 1.5 0.1 0.  0. ]\n",
      " [5.5 4.2 1.4 0.2 0.  0. ]\n",
      " [4.9 3.1 1.5 0.2 0.  0. ]\n",
      " [5.  3.2 1.2 0.2 0.  0. ]\n",
      " [5.5 3.5 1.3 0.2 0.  0. ]\n",
      " [4.9 3.6 1.4 0.1 0.  0. ]\n",
      " [4.4 3.  1.3 0.2 0.  0. ]\n",
      " [5.1 3.4 1.5 0.2 0.  0. ]\n",
      " [5.  3.5 1.3 0.3 0.  0. ]\n",
      " [4.5 2.3 1.3 0.3 0.  0. ]\n",
      " [4.4 3.2 1.3 0.2 0.  0. ]\n",
      " [5.  3.5 1.6 0.6 0.  0. ]\n",
      " [5.1 3.8 1.9 0.4 0.  0. ]\n",
      " [4.8 3.  1.4 0.3 0.  0. ]\n",
      " [5.1 3.8 1.6 0.2 0.  0. ]\n",
      " [4.6 3.2 1.4 0.2 0.  0. ]\n",
      " [5.3 3.7 1.5 0.2 0.  0. ]\n",
      " [5.  3.3 1.4 0.2 0.  0. ]\n",
      " [7.  3.2 4.7 1.4 1.  1. ]\n",
      " [6.4 3.2 4.5 1.5 1.  1. ]\n",
      " [6.9 3.1 4.9 1.5 2.  2. ]\n",
      " [5.5 2.3 4.  1.3 1.  1. ]\n",
      " [6.5 2.8 4.6 1.5 1.  1. ]\n",
      " [5.7 2.8 4.5 1.3 1.  1. ]\n",
      " [6.3 3.3 4.7 1.6 1.  1. ]\n",
      " [4.9 2.4 3.3 1.  1.  1. ]\n",
      " [6.6 2.9 4.6 1.3 1.  1. ]\n",
      " [5.2 2.7 3.9 1.4 1.  1. ]\n",
      " [5.  2.  3.5 1.  1.  1. ]\n",
      " [5.9 3.  4.2 1.5 1.  1. ]\n",
      " [6.  2.2 4.  1.  1.  1. ]\n",
      " [6.1 2.9 4.7 1.4 1.  1. ]\n",
      " [5.6 2.9 3.6 1.3 1.  1. ]\n",
      " [6.7 3.1 4.4 1.4 1.  1. ]\n",
      " [5.6 3.  4.5 1.5 1.  1. ]\n",
      " [5.8 2.7 4.1 1.  1.  1. ]\n",
      " [6.2 2.2 4.5 1.5 1.  1. ]\n",
      " [5.6 2.5 3.9 1.1 1.  1. ]\n",
      " [5.9 3.2 4.8 1.8 1.  1. ]\n",
      " [6.1 2.8 4.  1.3 1.  1. ]\n",
      " [6.3 2.5 4.9 1.5 1.  1. ]\n",
      " [6.1 2.8 4.7 1.2 1.  1. ]\n",
      " [6.4 2.9 4.3 1.3 1.  1. ]\n",
      " [6.6 3.  4.4 1.4 1.  1. ]\n",
      " [6.8 2.8 4.8 1.4 1.  1. ]\n",
      " [6.7 3.  5.  1.7 2.  2. ]\n",
      " [6.  2.9 4.5 1.5 1.  1. ]\n",
      " [5.7 2.6 3.5 1.  1.  1. ]\n",
      " [5.5 2.4 3.8 1.1 1.  1. ]\n",
      " [5.5 2.4 3.7 1.  1.  1. ]\n",
      " [5.8 2.7 3.9 1.2 1.  1. ]\n",
      " [6.  2.7 5.1 1.6 1.  1. ]\n",
      " [5.4 3.  4.5 1.5 1.  1. ]\n",
      " [6.  3.4 4.5 1.6 1.  1. ]\n",
      " [6.7 3.1 4.7 1.5 1.  1. ]\n",
      " [6.3 2.3 4.4 1.3 1.  1. ]\n",
      " [5.6 3.  4.1 1.3 1.  1. ]\n",
      " [5.5 2.5 4.  1.3 1.  1. ]\n",
      " [5.5 2.6 4.4 1.2 1.  1. ]\n",
      " [6.1 3.  4.6 1.4 1.  1. ]\n",
      " [5.8 2.6 4.  1.2 1.  1. ]\n",
      " [5.  2.3 3.3 1.  1.  1. ]\n",
      " [5.6 2.7 4.2 1.3 1.  1. ]\n",
      " [5.7 3.  4.2 1.2 1.  1. ]\n",
      " [5.7 2.9 4.2 1.3 1.  1. ]\n",
      " [6.2 2.9 4.3 1.3 1.  1. ]\n",
      " [5.1 2.5 3.  1.1 1.  1. ]\n",
      " [5.7 2.8 4.1 1.3 1.  1. ]\n",
      " [6.3 3.3 6.  2.5 2.  2. ]\n",
      " [5.8 2.7 5.1 1.9 1.  1. ]\n",
      " [7.1 3.  5.9 2.1 2.  2. ]\n",
      " [6.3 2.9 5.6 1.8 2.  2. ]\n",
      " [6.5 3.  5.8 2.2 2.  2. ]\n",
      " [7.6 3.  6.6 2.1 2.  2. ]\n",
      " [4.9 2.5 4.5 1.7 1.  1. ]\n",
      " [7.3 2.9 6.3 1.8 2.  2. ]\n",
      " [6.7 2.5 5.8 1.8 2.  2. ]\n",
      " [7.2 3.6 6.1 2.5 2.  2. ]\n",
      " [6.5 3.2 5.1 2.  2.  2. ]\n",
      " [6.4 2.7 5.3 1.9 2.  2. ]\n",
      " [6.8 3.  5.5 2.1 2.  2. ]\n",
      " [5.7 2.5 5.  2.  1.  1. ]\n",
      " [5.8 2.8 5.1 2.4 1.  1. ]\n",
      " [6.4 3.2 5.3 2.3 2.  2. ]\n",
      " [6.5 3.  5.5 1.8 2.  2. ]\n",
      " [7.7 3.8 6.7 2.2 2.  2. ]\n",
      " [7.7 2.6 6.9 2.3 2.  2. ]\n",
      " [6.  2.2 5.  1.5 1.  1. ]\n",
      " [6.9 3.2 5.7 2.3 2.  2. ]\n",
      " [5.6 2.8 4.9 2.  1.  1. ]\n",
      " [7.7 2.8 6.7 2.  2.  2. ]\n",
      " [6.3 2.7 4.9 1.8 1.  1. ]\n",
      " [6.7 3.3 5.7 2.1 2.  2. ]\n",
      " [7.2 3.2 6.  1.8 2.  2. ]\n",
      " [6.2 2.8 4.8 1.8 1.  1. ]\n",
      " [6.1 3.  4.9 1.8 1.  1. ]\n",
      " [6.4 2.8 5.6 2.1 2.  2. ]\n",
      " [7.2 3.  5.8 1.6 2.  2. ]\n",
      " [7.4 2.8 6.1 1.9 2.  2. ]\n",
      " [7.9 3.8 6.4 2.  2.  2. ]\n",
      " [6.4 2.8 5.6 2.2 2.  2. ]\n",
      " [6.3 2.8 5.1 1.5 1.  1. ]\n",
      " [6.1 2.6 5.6 1.4 2.  2. ]\n",
      " [7.7 3.  6.1 2.3 2.  2. ]\n",
      " [6.3 3.4 5.6 2.4 2.  2. ]\n",
      " [6.4 3.1 5.5 1.8 2.  2. ]\n",
      " [6.  3.  4.8 1.8 1.  1. ]\n",
      " [6.9 3.1 5.4 2.1 2.  2. ]\n",
      " [6.7 3.1 5.6 2.4 2.  2. ]\n",
      " [6.9 3.1 5.1 2.3 2.  2. ]\n",
      " [5.8 2.7 5.1 1.9 1.  1. ]\n",
      " [6.8 3.2 5.9 2.3 2.  2. ]\n",
      " [6.7 3.3 5.7 2.5 2.  2. ]\n",
      " [6.7 3.  5.2 2.3 2.  2. ]\n",
      " [6.3 2.5 5.  1.9 1.  1. ]\n",
      " [6.5 3.  5.2 2.  2.  2. ]\n",
      " [6.2 3.4 5.4 2.3 2.  2. ]\n",
      " [5.9 3.  5.1 1.8 1.  1. ]]\n",
      "iterations---> 4\n"
     ]
    }
   ],
   "source": [
    "# k-means clustering algo\n",
    "import math\n",
    "import numpy as np\n",
    "from scipy.spatial import distance\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "def k_means(num_clusters, df, init_centres):\n",
    "\n",
    "    #returns a list of k initial centre points for cluster initialization\n",
    "    def choose_initial_centres(num_clusters, df):\n",
    "        num_docs = len(df)\n",
    "        every_x_item = num_docs/num_clusters\n",
    "        df_centres = df[::math.ceil(every_x_item)]\n",
    "        return df_centres.iloc[:num_clusters]\n",
    "\n",
    "    #one time initialization steps    \n",
    "    #add new columns in dataframe that would contain distance values from the point to the centre\n",
    "    # df1 = pd.DataFrame(columns=list(map(lambda x: \"dist_c\"+str(x), range(num_clusters))))\n",
    "    # df = df.join(df1, how=\"outer\")\n",
    "    # print(df)\n",
    "\n",
    "    #choose centre points from data if not already given\n",
    "    if(len(init_centres)!=num_clusters):\n",
    "        centres = choose_initial_centres(num_clusters, df)\n",
    "\n",
    "    #determine distance of points from the centres and assign clusters\n",
    "    # df = shuffle(df)\n",
    "    data = df.to_numpy()\n",
    "    centroids = centres.to_numpy()\n",
    "    data = np.hstack([data, np.zeros((len(data),1)), np.ones((len(data),1))])\n",
    "    #compute the centroids till the cluster assignment remains the same\n",
    "    fit(data, centroids, num_clusters)\n",
    "\n",
    "#takes in a dataframe and a center vector and outputs a series with distance values of all points from the vector\n",
    "def fit(data, centroids, num_clusters):\n",
    "    iter = 0\n",
    "    print(centroids)\n",
    "    while(not np.array_equiv(data[:,5],data[:,4])):\n",
    "        if(iter!=0):\n",
    "            data[:,4] = data[:,5]\n",
    "            centroids = update_centroids(num_clusters, data)\n",
    "        iter+=1\n",
    "        dist = []\n",
    "        for point in data:\n",
    "            dist_val = []\n",
    "            for center in centroids:\n",
    "                eucledian_dist = distance.euclidean(point[0:4], center)\n",
    "                dist_val.append(eucledian_dist)\n",
    "            cluster_val = dist_val.index(min(dist_val))\n",
    "            dist.append(cluster_val)\n",
    "        data[:,5] = dist\n",
    "    print(data)\n",
    "    print('iterations--->',iter)\n",
    "\n",
    "    #handles case where no point is assigned to a cluster center\n",
    "    #takes in all data points assigned to individual clusters and asks for clustering again with new centroids\n",
    "def update_centroids(num_clusters, data):\n",
    "    #num_columns = len(df.shape[1])\n",
    "    #make this dynamic: take as many columns as are there in the dataframe\n",
    "    # clusters_with_no_pts = df[:4].nunique() - num_clusters\n",
    "    #choose random samples as cluster centres\n",
    "    # if (clusters_with_no_pts>0):\n",
    "    #     centers = df.sample(n=clusters_with_no_pts)\n",
    "\n",
    "    #find mean by cluster value and call eucledian distance again\n",
    "    centroids = []\n",
    "    c0 = data[data[:,4]== 0.0]\n",
    "    c1 = data[data[:,4]== 1.0]\n",
    "    c2 = data[data[:,4]== 2.0]\n",
    "\n",
    "    center_0 = np.mean(c0[:,0:4], axis=0)\n",
    "    center_1 = np.mean(c1[:,0:4], axis=0)\n",
    "    center_2 = np.mean(c2[:,0:4], axis=0)\n",
    "\n",
    "    print('center_0-->',center_0)\n",
    "    print('center_1-->',center_1)\n",
    "    print('center_2-->',center_2)\n",
    "\n",
    "    # for i in range(num_clusters):\n",
    "    #     centroids = center_0\n",
    "\n",
    "    centroids = [center_0, center_1, center_2]\n",
    "    return centroids\n",
    "\n",
    "    # mask_list = []\n",
    "    # for mask_no in range(num_clusters):\n",
    "    #     mask_list.append(df['cluster'] == mask_no)\n",
    "\n",
    "    # df_with_centroids = [] \n",
    "    # for mask in mask_list:\n",
    "    #     df_with_centroids.append(df[mask])\n",
    "\n",
    "    # centers_df = pd.DataFrame([])\n",
    "    # for dataframe in df_with_centroids:\n",
    "    #     centers_df.concat(dataframe.mean(axis=1))\n",
    "\n",
    "    # print(centers_df)\n",
    "    # return centers_df\n",
    "#penalize point for not being assigned to must link peers:\n",
    "def penalize(point, postive_sentiment_set, negative_sentiment_set, must_link_penalty, cannot_link_penalty):\n",
    "    point_cluster = point[4]\n",
    "    penalty = 0.0\n",
    "\n",
    "    #return zero penalty for negative sentiment documents\n",
    "    if point['sentiment'] == 0:\n",
    "        return penalty\n",
    "        \n",
    "    #class comparision\n",
    "    elif point['sentiment'] == 1:\n",
    "        # no_effect = postive_sentiment_set\n",
    "        # cannot_link_set = negative_sentiment_set\n",
    "    \n",
    "    # else:\n",
    "    #     must_link_set \n",
    "    #     cannot_link_set \n",
    "    \n",
    "    for ml_pt in must_link_set:\n",
    "        if ml_pt != point and ml_pt_cluster!= -1 and point_cluster != ml_pt_cluster:\n",
    "            penalty += must_link_penalty\n",
    "\n",
    "    for cl_pt in cannot_link_set:\n",
    "        if cl_pt !=point and cl_pt_cluster!= -1 and point_cluster == cl_pt_cluster:\n",
    "            penalty += cannot_link_penalty\n",
    "\n",
    "    return penalty\n",
    "\n",
    "#constraint filtering: post k-means\n",
    "\n",
    "#constraints: must link [1,3,5, 4,2,5]\n",
    "# k_means(5, df, [], must_link, cannot_link)\n",
    "# assign initial centres\n",
    "# assigns clusters to -1\n",
    "# calculates cluster assignment\n",
    "# prev_clusters = initial clusters\n",
    "# while (calculated_clusters = prev_clusters)\n",
    "#   calculate_new_centroids\n",
    "#   calculate_eucledian_dist\n",
    "#   penalize constraints\n",
    "#   new cluster assignment\n",
    "k_means(3, df, [])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fd99746d5d03835a040f73f9b97c9addf66f20a4c98010bfbcb520836dc1f894"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
