{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load toy set data\n",
    "# from sklearn import datasets\n",
    "# iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neutral docs--> (32594, 121)\n",
      "positive docs--> (14283, 121)\n",
      "negative docs--> (10637, 121)\n",
      "                      0_x       1_x       2_x       3_x       4_x       5_x  \\\n",
      "docno                                                                         \n",
      " LA122989-0011   0.625603  0.386697  0.479026  0.412594  0.391509  0.458001   \n",
      " LA122989-0012   0.646113  0.436381  0.513598  0.389201  0.457487  0.522667   \n",
      " LA122989-0018   0.606507  0.552505  0.513218  0.526657  0.270074  0.364127   \n",
      " LA122989-0019   0.560057  0.674291  0.583621  0.480622  0.283424  0.271703   \n",
      " LA122989-0021   0.466378  0.637155  0.603542  0.431422  0.253742  0.312299   \n",
      "...                   ...       ...       ...       ...       ...       ...   \n",
      "FT911-3655       0.399649  0.504588  0.290757  0.565279  0.336671  0.493825   \n",
      "FT911-3656       0.436370  0.505907  0.408461  0.497220  0.362383  0.384115   \n",
      "FT911-3669       0.381024  0.454289  0.428082  0.466558  0.367593  0.462460   \n",
      "FT911-3679       0.378951  0.463879  0.370378  0.528692  0.375063  0.473736   \n",
      "FT911-3693       0.452431  0.497637  0.344397  0.665750  0.362865  0.448901   \n",
      "\n",
      "                      6_x       7_x       8_x       9_x  ...      11_y  \\\n",
      "docno                                                    ...             \n",
      " LA122989-0011   0.385795  0.418231  0.470648  0.544261  ...  0.000000   \n",
      " LA122989-0012   0.404469  0.476440  0.539464  0.449317  ...  0.000000   \n",
      " LA122989-0018   0.258842  0.385201  0.455938  0.703980  ...  0.093412   \n",
      " LA122989-0019   0.150597  0.461255  0.570351  0.605260  ...  0.038406   \n",
      " LA122989-0021   0.220842  0.328217  0.582143  0.660673  ...  0.000000   \n",
      "...                   ...       ...       ...       ...  ...       ...   \n",
      "FT911-3655       0.506511  0.489812  0.431916  0.492070  ...  0.325047   \n",
      "FT911-3656       0.612473  0.434565  0.443487  0.488501  ...  0.277032   \n",
      "FT911-3669       0.617601  0.295972  0.457131  0.596846  ...  0.000000   \n",
      "FT911-3679       0.586433  0.469959  0.445982  0.524574  ...  0.064257   \n",
      "FT911-3693       0.469553  0.509169  0.603531  0.405118  ...  0.135494   \n",
      "\n",
      "                     12_y      13_y      14_y      15_y      16_y      17_y  \\\n",
      "docno                                                                         \n",
      " LA122989-0011   0.000000  0.000000  0.026833  0.000000  0.000000  0.000000   \n",
      " LA122989-0012   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      " LA122989-0018   0.028161  0.020716  0.000000  0.000000  0.000000  0.484554   \n",
      " LA122989-0019   0.000000  0.000000  0.000000  0.000000  0.000000  0.178057   \n",
      " LA122989-0021   0.000000  0.000000  0.000000  0.000000  0.000000  0.158235   \n",
      "...                   ...       ...       ...       ...       ...       ...   \n",
      "FT911-3655       0.045473  0.015866  0.000000  0.000000  0.047221  0.000000   \n",
      "FT911-3656       0.013543  0.093534  0.000000  0.000000  0.011644  0.000000   \n",
      "FT911-3669       0.037298  0.101842  0.015265  0.000000  0.128832  0.000000   \n",
      "FT911-3679       0.167407  0.185229  0.000000  0.033993  0.000000  0.000000   \n",
      "FT911-3693       0.016239  0.577288  0.103913  0.000000  0.000000  0.000000   \n",
      "\n",
      "                     18_y      19_y  class  \n",
      "docno                                       \n",
      " LA122989-0011   0.000000  0.000000      1  \n",
      " LA122989-0012   0.000000  0.000000      1  \n",
      " LA122989-0018   0.137688  0.000000      1  \n",
      " LA122989-0019   0.000000  0.000000      1  \n",
      " LA122989-0021   0.000000  0.000000      1  \n",
      "...                   ...       ...    ...  \n",
      "FT911-3655       0.000000  0.196846      1  \n",
      "FT911-3656       0.000000  0.014818      1  \n",
      "FT911-3669       0.000000  0.081275      1  \n",
      "FT911-3679       0.000000  0.117079      1  \n",
      "FT911-3693       0.000000  0.000000      1  \n",
      "\n",
      "[14283 rows x 121 columns]\n"
     ]
    }
   ],
   "source": [
    "#load data in dataframes\n",
    "import pandas as pd\n",
    "doc_path = \"../feature_extraction/word2vec_LDA_sentiment.csv\"\n",
    "df=pd.read_csv(doc_path,index_col='docno')\n",
    "pos_docs = df[df[\"class\"] == 1]\n",
    "neg_docs = df[df[\"class\"] == -1]\n",
    "neu_docs = df[df[\"class\"] == 0]\n",
    "print('neutral docs-->',neu_docs.shape)\n",
    "print('positive docs-->',pos_docs.shape)\n",
    "print('negative docs-->',neg_docs.shape)\n",
    "print(pos_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2814763704.py, line 103)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/var/folders/n_/nq9ht6c94dd4qtlf4fk_mwsw0000gn/T/ipykernel_71604/2814763704.py\"\u001b[0;36m, line \u001b[0;32m103\u001b[0m\n\u001b[0;31m    postive_sentiment_set =\u001b[0m\n\u001b[0m                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from scipy.spatial import distance\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "def k_means(num_clusters, df, init_centres, pos_docs, neg_docs, neu_docs, must_link_penalty, cannot_link_penalty):\n",
    "\n",
    "    #returns a list of k initial centre points for cluster initialization\n",
    "    def choose_initial_centres(num_clusters, df):\n",
    "        num_docs = len(df)\n",
    "        every_x_item = num_docs/num_clusters\n",
    "        df_centres = df[::math.ceil(every_x_item)]\n",
    "        return df_centres.iloc[:num_clusters]\n",
    "\n",
    "    #one time initialization steps    \n",
    "    #add new columns in dataframe that would contain distance values from the point to the centre\n",
    "    # df1 = pd.DataFrame(columns=list(map(lambda x: \"dist_c\"+str(x), range(num_clusters))))\n",
    "    # df = df.join(df1, how=\"outer\")\n",
    "    # print(df)\n",
    "\n",
    "    #choose centre points from data if not already given\n",
    "    if(len(init_centres)!=num_clusters):\n",
    "        num_centres = num_clusters/3\n",
    "        rem_centres = num_clusters%3\n",
    "        pos_centers = choose_initial_centres(num_centres+rem_centres, pos_docs)\n",
    "        neg_centers = choose_initial_centres(num_centres, neg_docs)\n",
    "        neu_centres = choose_initial_centres(num_centres, neu_docs)\n",
    "        centres = centres.append(pos_centers, neg_centers, neu_centres)\n",
    "        print(centres)\n",
    "        \n",
    "    #determine distance of points from the centres and assign clusters\n",
    "    # df = shuffle(df)\n",
    "    data = df.to_numpy()\n",
    "    centroids = centres.to_numpy()\n",
    "    data = np.hstack([data, np.zeros((len(data),1)), np.ones((len(data),1))])\n",
    "    #compute the centroids till the cluster assignment remains the same\n",
    "    fit(data, centroids, num_clusters, pos_docs, neg_docs, must_link_penalty, cannot_link_penalty)\n",
    "\n",
    "#takes in a dataframe and a center vector and outputs a series with distance values of all points from the vector\n",
    "def fit(data, centroids, num_clusters, pos_docs, neg_docs, must_link_penalty, cannot_link_penalty):\n",
    "    iter = 0\n",
    "    print(centroids)\n",
    "    while(not np.array_equiv(data[:,5],data[:,4])):\n",
    "        if(iter!=0):\n",
    "            data[:,4] = data[:,5]\n",
    "            centroids = update_centroids(num_clusters, data)\n",
    "        iter+=1\n",
    "        dist = []\n",
    "        for point in data:\n",
    "            dist_val = []\n",
    "            for index, center in centroids:\n",
    "                eucledian_dist = distance.euclidean(point[0:4], center)\n",
    "                penalty_dist = penalize(point, index, pos_docs, neg_docs, must_link_penalty, cannot_link_penalty)\n",
    "                dist_val.append(eucledian_dist+penalty_dist)\n",
    "            cluster_val = dist_val.index(min(dist_val))\n",
    "            dist.append(cluster_val)\n",
    "        data[:,5] = dist\n",
    "    print(data)\n",
    "    print('iterations--->',iter)\n",
    "\n",
    "\n",
    "# penalize point for not being assigned to must link peers and being assigned to cannot link peers:\n",
    "def penalize(point, assumed_pt_cluster, pos_docs, neg_docs, must_link_penalty, cannot_link_penalty):\n",
    "    penalty = 0.0\n",
    "\n",
    "    #return zero penalty for neutral sentiment documents\n",
    "    if point['class'] == 0:\n",
    "        return penalty\n",
    "        \n",
    "    elif point['class'] == 1:\n",
    "        must_link_set = pos_docs\n",
    "        cannot_link_set = neg_docs\n",
    "    \n",
    "    else:\n",
    "        must_link_set = neg_docs\n",
    "        cannot_link_set = pos_docs\n",
    "    \n",
    "    #return negative penalty for neutral sentiment documents\n",
    "    for ml_pt in must_link_set:\n",
    "        if ml_pt !=point and ml_pt[4]!= -1 and assumed_pt_cluster != ml_pt[4]:\n",
    "            penalty += must_link_penalty\n",
    "\n",
    "    #return positive penalty for  sentiment documents\n",
    "    for cl_pt in cannot_link_set:\n",
    "        if cl_pt !=point and cl_pt[4] != -1 and assumed_pt_cluster == cl_pt[4]:\n",
    "            penalty += cannot_link_penalty\n",
    "\n",
    "    return penalty\n",
    "\n",
    "    #handles case where no point is assigned to a cluster center\n",
    "    #takes in all data points assigned to individual clusters and asks for clustering again with new centroids\n",
    "def update_centroids(num_clusters, data):\n",
    "    #num_columns = len(df.shape[1])\n",
    "    #make this dynamic: take as many columns as are there in the dataframe\n",
    "    # clusters_with_no_pts = df[:4].nunique() - num_clusters\n",
    "    #choose random samples as cluster centres\n",
    "    # if (clusters_with_no_pts>0):\n",
    "    #     centers = df.sample(n=clusters_with_no_pts)\n",
    "\n",
    "    #find mean by cluster value and call eucledian distance again\n",
    "    centroids = []\n",
    "    c0 = data[data[:,4]== 0.0]\n",
    "    c1 = data[data[:,4]== 1.0]\n",
    "    c2 = data[data[:,4]== 2.0]\n",
    "\n",
    "    center_0 = np.mean(c0[:,0:4], axis=0)\n",
    "    center_1 = np.mean(c1[:,0:4], axis=0)\n",
    "    center_2 = np.mean(c2[:,0:4], axis=0)\n",
    "\n",
    "    print('center_0-->',center_0)\n",
    "    print('center_1-->',center_1)\n",
    "    print('center_2-->',center_2)\n",
    "\n",
    "    # for i in range(num_clusters):\n",
    "    #     centroids = center_0\n",
    "\n",
    "    centroids = [center_0, center_1, center_2]\n",
    "    return centroids\n",
    "\n",
    "    # mask_list = []\n",
    "    # for mask_no in range(num_clusters):\n",
    "    #     mask_list.append(df['cluster'] == mask_no)\n",
    "\n",
    "    # df_with_centroids = [] \n",
    "    # for mask in mask_list:\n",
    "    #     df_with_centroids.append(df[mask])\n",
    "\n",
    "    # centers_df = pd.DataFrame([])\n",
    "    # for dataframe in df_with_centroids:\n",
    "    #     centers_df.concat(dataframe.mean(axis=1))\n",
    "\n",
    "    # print(centers_df)\n",
    "    # return centers_df\n",
    "\n",
    "\n",
    "#constraint filtering: post k-means\n",
    "\n",
    "#constraints: must link [1,3,5, 4,2,5]\n",
    "# k_means(5, df, [], must_link, cannot_link)\n",
    "# assign initial centres\n",
    "# assigns clusters to -1\n",
    "# calculates cluster assignment\n",
    "# prev_clusters = initial clusters\n",
    "# while (calculated_clusters = prev_clusters)\n",
    "#   calculate_new_centroids\n",
    "#   calculate_eucledian_dist\n",
    "#   penalize constraints\n",
    "#   new cluster assignment\n",
    "k_means(3, df, [])\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fd99746d5d03835a040f73f9b97c9addf66f20a4c98010bfbcb520836dc1f894"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
