{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load toy set data\n",
    "# from sklearn import datasets\n",
    "# iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from scipy.spatial import distance\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# NOTE(Abid): For debugging purposes only\n",
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "def k_means(num_clusters, df, keyphrase_df, init_centres, pos_docs, neg_docs, neu_docs, must_link_penalty, cannot_link_penalty):\n",
    "\n",
    "    #returns a list of k initial centre points for cluster initialization\n",
    "    def choose_initial_centres(num_clusters, df):\n",
    "        num_docs = len(df)\n",
    "        every_x_item = num_docs/num_clusters\n",
    "        df_centres = df[::math.ceil(every_x_item)]\n",
    "        return df_centres.iloc[:num_clusters]\n",
    "\n",
    "    #choose centre points from data if not already given\n",
    "    if(len(init_centres)!=num_clusters):\n",
    "        centres = []\n",
    "        num_centres = math.floor(num_clusters/3)\n",
    "        rem_centres = num_clusters%3\n",
    "        pos_centers = choose_initial_centres(num_centres+rem_centres, pos_docs)\n",
    "        neg_centers = choose_initial_centres(num_centres, neg_docs)\n",
    "        neu_centres = choose_initial_centres(num_centres, neu_docs)\n",
    "        centres = np.concatenate((pos_centers, neg_centers, neu_centres))\n",
    "    \n",
    "    #determine distance of points from the centres and assign clusters\n",
    "    # df = shuffle(df)\n",
    "    data = df.to_numpy()\n",
    "    col_count = len(data[:][0])\n",
    "    centroids = centres[:,:col_count-1]\n",
    "    data = np.hstack([data, np.zeros((len(data),1)), np.ones((len(data),1))])\n",
    "    #compute the centroids till the cluster assignment remains the same\n",
    "    fit(data, df, keyphrase_df, col_count, pos_docs, neg_docs, neu_docs, centroids, num_clusters, must_link_penalty, cannot_link_penalty)\n",
    "\n",
    "def extract_sentiment_index(df, pos_docs, neg_docs,  neu_docs):\n",
    "    pos_docs_loc = df.index.get_indexer(pos_docs.index.to_list())\n",
    "    neg_docs_loc = df.index.get_indexer(neg_docs.index.to_list())\n",
    "    neu_docs_loc = df.index.get_indexer(neu_docs.index.to_list())\n",
    "    return pos_docs_loc, neg_docs_loc, neu_docs_loc\n",
    "\n",
    "#takes in a dataframe and a center vector and outputs a series with distance values of all points from the vector\n",
    "#last 2 columns reserved for cluster comparision (current cluster and prev cluster)\n",
    "def fit(data, df, keyphrase_df, col_count, pos_docs, neg_docs, neu_docs, centroids, num_clusters, must_link_penalty, cannot_link_penalty):\n",
    "    iter = 0\n",
    "    pos_docs_loc, neg_docs_loc, neu_docs_loc = extract_sentiment_index(df, pos_docs, neg_docs, neu_docs)\n",
    "    # print(data[:,col_count+1])\n",
    "    # print(col_count)\n",
    "    # print(data[:,:col_count+1])\n",
    "    \n",
    "    while(not np.array_equiv(data[:,col_count+1],data[:,col_count])):\n",
    "        if(iter!=0):\n",
    "            data[:,col_count] = data[:,col_count+1]\n",
    "            data[:,col_count+1] = -1\n",
    "            centroids = update_centroids(num_clusters, data[:,:col_count+1],col_count)\n",
    "        iter+=1\n",
    "        for row_index, point in enumerate(data):\n",
    "            dist_val = []\n",
    "            for index, center in enumerate(centroids):\n",
    "                eucledian_dist = distance.euclidean(point[:col_count-1], center)\n",
    "                penalty_dist = penalize(point, data, col_count, index, pos_docs_loc, neg_docs_loc, must_link_penalty, cannot_link_penalty)\n",
    "                penalty_keyphrase = penalize_keyphrase(df, data, keyphrase_df, col_count, row_index, index, np.array([0., 0.1]))\n",
    "                dist_val.append(eucledian_dist+penalty_dist+penalty_keyphrase)\n",
    "            cluster_val = dist_val.index(min(dist_val))\n",
    "            data[row_index,col_count+1] = cluster_val\n",
    "        print('iteration count--->',iter)\n",
    "    return data\n",
    "    # pd.DataFrame(data).to_csv('./sentiment_clusters.csv')\n",
    "\n",
    "# penalize point for not being assigned to must link peers and being assigned to cannot link peers:\n",
    "def penalize(point, data, col_count, assumed_pt_cluster, pos_docs_loc, neg_docs_loc, must_link_penalty, cannot_link_penalty):\n",
    "    \n",
    "    penalty = 0.0\n",
    "\n",
    "    #return zero penalty for neutral sentiment documents\n",
    "    if point[col_count-1] == 0:\n",
    "        return penalty\n",
    "        \n",
    "    elif point[col_count-1] == 1:\n",
    "        must_link_set = pos_docs_loc\n",
    "        cannot_link_set = neg_docs_loc\n",
    "    \n",
    "    else:\n",
    "        must_link_set = neg_docs_loc\n",
    "        cannot_link_set = pos_docs_loc\n",
    "    \n",
    "    #return negative penalty for neutral sentiment documents\n",
    "    for ml_pt in must_link_set:\n",
    "        if np.any(np.not_equal(data[ml_pt][:col_count-1],point[:col_count-1])) and data[ml_pt][col_count+1] != -1 and assumed_pt_cluster == data[ml_pt][col_count+1]:\n",
    "            penalty += must_link_penalty\n",
    "\n",
    "    #return positive penalty for  sentiment documents\n",
    "    for cl_pt in cannot_link_set:\n",
    "        if np.any(np.not_equal(data[cl_pt][:col_count-1],point[:col_count-1])) and data[cl_pt][col_count+1] != -1 and assumed_pt_cluster == data[cl_pt][col_count+1]:\n",
    "            penalty += cannot_link_penalty\n",
    "\n",
    "    return penalty\n",
    "\n",
    "# NOTE(Abid): I've decided to not give negative penalty if the keyphrases do not much, based on the reasoning that a cluster will \n",
    "#             almost always have more topics that are different from the point than those that are similar to it.\n",
    "def penalize_keyphrase(df, data, keyphrase_df, col_count, point_idx, potential_cluster_val, penalty_vals):\n",
    "    penalty = 0.0\n",
    "    \n",
    "    point_keyphrases = (keyphrase_df.iloc[point_idx,:]).to_numpy()\n",
    "    for idx in range(len(data[:,0])):\n",
    "        test_point = df.iloc[idx,:]\n",
    "        if point_idx == idx:\n",
    "            continue\n",
    "        \n",
    "        if data[idx, col_count+1] != -1 and data[idx, col_count+1] == potential_cluster_val:\n",
    "            test_keyphrases = np.array([str(keyphrase_df.iloc[idx,0]), str(keyphrase_df.iloc[idx,2])])\n",
    "            test_weights = np.array([float(keyphrase_df.iloc[idx,1]), float(keyphrase_df.iloc[idx,3])])\n",
    "            keyphrases = np.array([point_keyphrases[0], point_keyphrases[2]])\n",
    "            weights = np.array([point_keyphrases[1], point_keyphrases[3]])\n",
    "            for pen_idx in range(len(penalty_vals)):\n",
    "                keyphrase = keyphrases[pen_idx]\n",
    "                weight = weights[pen_idx]\n",
    "                first_word = keyphrase.split(\" \")[0]\n",
    "                if first_word == \"<None>\" or len(first_word) == 0:\n",
    "                    return penalty\n",
    "                if first_word == test_keyphrases[0].split(\" \")[0]:\n",
    "#                     set_trace()\n",
    "                    penalty += ((weight+test_weights[0])/2) * penalty_vals[pen_idx]\n",
    "                elif first_word == test_keyphrases[1].split(\" \")[0]:\n",
    "                    penalty += ((weight+test_weights[1])/2) * penalty_vals[pen_idx]\n",
    "#                 else:\n",
    "#                     penalty -= 0.5((weight+test_weights[pen_idx])/2) * penalty_vals[pen_idx]\n",
    "    return penalty\n",
    "\n",
    "#handles case where no point is assigned to a cluster center\n",
    "def update_centroids(num_clusters, data, col_count):\n",
    "    \n",
    "    centroids = []\n",
    "    data_per_cluster = []\n",
    "    for clus_no in range(num_clusters):\n",
    "        data_per_cluster.append(data[data[:,col_count]== float(clus_no)])\n",
    "    \n",
    "    for cluster_data in data_per_cluster:\n",
    "        centroids.append(np.mean(cluster_data[:,:col_count-1], axis=0))\n",
    "    \n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data in dataframes\n",
    "import pandas as pd\n",
    "doc_path = \"../feature_extraction/word2vec_LDA_sentiment.csv\"\n",
    "df=pd.read_csv(doc_path,index_col='docno')\n",
    "df = df.iloc[:150,:]\n",
    "\n",
    "keyphrase_df = pd.read_csv(\"keyphrase_docno_added.csv\",index_col='docno')\n",
    "# print(df.shape)\n",
    "pos_doc_df = df[df[\"class\"] == 1]\n",
    "neg_doc_df = df[df[\"class\"] == -1]\n",
    "neu_doc_df = df[df[\"class\"] == 0]\n",
    "k_means(5, df, keyphrase_df, [], pos_doc_df, neg_doc_df, neu_doc_df, -0.2, 0.4)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fd99746d5d03835a040f73f9b97c9addf66f20a4c98010bfbcb520836dc1f894"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
